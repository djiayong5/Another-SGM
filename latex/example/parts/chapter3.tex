\chapter{Сравнение с методами многокритериальной оптимизации} 
\label{chapter3}

Существуют методы, в которых задача скалярной оптимизации сводится к задаче многокритериальной оптимизации с целью устранения остановки поиска наиболее приспособленных особей эволюционного алгоритма в локальном оптимуме, а также поддержания разнообразия особей в популяции \cite{mh-iff, single-from-multi, helpers}. Сведение к многокритериальной задаче предполагает разработку специальных дополнительных критериев, гарантированно коррелирующих с целевой ФП. В задаче, поставленной в настоящей работе, заранее не известно, коррелируют ли вспомогательные функции с целевой. Методы многокритериальной оптимизации не эффективны в случае, когда среди вспомогательных функций есть функции, не коррелирующие с целевой.

Разработанный метод может применяться вместо методов многокритериальной оптимизации в случае, когда они используются для решения скалярной задачи оптимизации. Для этого в качестве вспомогательных ФП следует использовать введенные на этапе сведения к многокритериальности дополнительные критерии. В данном разделе приведено экспериментальное сравнение разработанного метода с методами многокритериальной оптимизации на примере решения модельной задачи. Показано, что предлагаемый метод позволяет не учитывать невыгодные критерии, что приводит к большей производительности разработанного метода по сравнению с методами многокритериальной оптимизации.
 
\section{Модельная задача H-IFF}
\label{h-iff}

Рассмотрим функцию, называемую Hierarchical-if-and-only-if function, H-IFF \cite{h-iff, mh-iff}. Максимизация этой функции, применяемой в качестве ФП, является модельной задачей, используемой для тестирования генетических алгоритмов. Пространство поиска решений этой задачи состоит из битовых строк фиксированной длины $l$. Функция принимает на вход битовую строку $B = b_1b_2\ldots b_l$ и интерпретирует ее как двоичное дерево, узлами и листьями которого являются строковые блоки. Вершине дерева соответствует исходная строка $B$, потомки каждого узла являются левой $B_L$ и правой $B_R$ половинами родительского блока. ФП $f$ вычисляется как сумма длин однородных блоков, состоящих только из нулей или только из единиц. Соответствующая рекурсивная формула приведена в (\ref{f}). Обратите внимание, что существует два возможных оптимальных решения задачи. Один оптимум представляет собой строку из нулевых битов, другой оптимум имеет вид строки, состоящей из единичных битов. 
\begin{equation}
\label{f}
      f(B) =
      \begin{cases}
       1 & \text{если } |B| = 1\text{, иначе}\\
       |B| + f(B_L) + f(B_R) & \text{если } \forall i \{b_i = 0\}\text{ или }\forall i \{b_i = 1\}\\
       f(B_L) + f(B_R) & \text{в остальных случаях}
      \end{cases}
\end{equation}

Различные методы скалярной оптимизации останавливаются в локальном оптимуме, решая эту задачу. Однако задача может быть решена с использованием методов многокритериальной оптимизации после предварительного введения нескольких критериев \cite{mh-iff}. Эти методы используют два критерия $f_0$ и $f_1$, которые учитывают только те блоки, которые состоят из нулей, или только те блоки, которые состоят из единиц, соответственно (\ref{fn}). Таким образом, получаем модифицированную задачу, называемую MH-IFF.
\begin{equation}
\label{fn}
      f_n(B) =
      \begin{cases}
       0 & \text{если } |B| = 1\text{ и }b_1 \neq n\text{, иначе}\\
       1 & \text{если } |B| = 1\text{ и }b_1 = n\text{, иначе}\\
       |B| + f_n(B_L) + f_n(B_R) & \text{if } \forall i \{b_i = n\}\\
       f_n(B_L) + f_n(B_R) & \text{в остальных случаях}
      \end{cases}
\end{equation}

\section{Решение задачи H-IFF с помощью разработанного метода}

Рассмотрим применение разработанного метода EA~+~RL к решению задачи H-IFF. Представим эту задачу как задачу скалярной оптимизации со вспомогательными критериями (H-IFFA).

Будем использовать функцию $f$ (\ref{f}) в качестве целевого критерия: 
$$g = f: W \rightarrow \mathbb{R}, W = \{B: |B| \leq l\}.$$ 
Функции $f_0$ и $f_1$ (\ref{fn}) будем использовать в качестве вспомогательных:
$$H = \{f_0, f_1\},  f_i : W \rightarrow \mathbb{R}.$$ 
Целью решения задачи является максимизации функции $f$ на множестве всех битовых строк фиксированной длины $l$: 
$$f(x) \rightarrow \max_{x \in X}, X =\{B: |B| = l\}$$

Каждое решение поставленной задачи H-IFFA является решением задачи H-IFF и наоборот. Задача H-IFFA может быть решена с помощью метода EA~+~RL. Для этого достаточно определить задачу обучения с подкреплением в соответствии с формулами (\ref{act}), (\ref{state}), (\ref{reward}). Таким образом, получаем новый способ решения задачи H-IFF. Результаты эксперимента по использованию этого способа представлены в следующем разделе. 

\section{Описание эксперимента}

В ходе эксперимента были решены задачи H-IFF и H-IFFA с помощью соответствующих методов. Параметры эксперимента соответствовали параметрам, использованным в статье \cite{mh-iff}, в которой приведены результаты решения задач H-IFF и MH-IFF. Длина особи $l$ составляла 64 бита. Заметим, что максимальное значение приспособленности такой особи составляет 448. Было произведено 30 запусков каждого из использованных алгоритмов. В каждом из запусков было выполнено 500000 вычислений ФП. Для сбора приведенной далее статистики использовались лучшие особи последнего поколения каждого из запусков. 

Задача H-IFF решалась с использованием двух различных типов ЭА, а именно с применением генетического алгоритма (ГА) и эволюционной стратегии (ЭС). Опишем сначала параметры ГА. Использовался одноточечный кроссовер \cite{mitchell-ga}, применявшийся с вероятностью 70\%. Оператор мутации, использовавшийся в ГА, инвертировал каждый бит каждой особи с вероятностью $2 / l = 3.125\%$. В качестве механизма селекции был применен турнирный отбор с вероятностью выбора лучшей особи, равной 90\%. Пять лучших особей каждого поколения ГА переносились в следующее поколение согласно стратегии элитизма. 

В ЭС использовался механизм селекции $(1 + m), m = 1, 5, 10.$ Оператор мутации инвертировал один случайно выбранный бит каждой особи. Эта вариация ЭС подвержена остановке в локальном оптимуме. Одной из целей эксперимента было показать, что разработанный метод EA~+~RL способен эффективно настраивать такой алгоритм, позволяя находить с его помощью глобальный оптимум.

Задача H-IFFA решалась с помощью разработанного метода, контролировавшего описанные выше ЭА. Значения параметров различных использованных алгоритмов обучения приведены в табл. \ref{param-learning}. Они настраивались вручную таким образом, чтобы результаты применения этих алгоритмов к решению задачи H-IFFA были наилучшими из полученных. В алгоритмах, не специфицирующих стратегию исследования среды, была применена $\varepsilon$-жадная стратегия (\ref{strategy}).

Также для обеспечения более основательного сравнения был реализован алгоритм многокритериальной оптимизации PESA-II \cite{pesa-ii}. Он применялся как для решения задачи MH-IFF в классической постановке, так и для решения задачи MH-IFF с добавленной \emph{мешающей} ФП, которая подсчитывала число совпадений с битовой маской длины $l$, состоявшей из чередующихся нулей и единиц: $1010...10$. Можно видеть, что оптимизация по этой функции не приводит к росту значений целевой функции H-IFF. Задача с мешающей ФП была также решена с применением разработанного метода EA~+~RL. Целью этой части эксперимента было показать, что применение обучения позволяет не учитывать мешающие ФП и по-прежнему достигать глобального оптимума, в то время как результаты, получаемые с использованием многокритериальной оптимизации, при наличии таких ФП ухудшаются.

\begin{table}[ht]
\begin{center}
\caption{Значения параметров алгоритмов обучения} \label{param-learning}
\begin{tabular}{|l|l|l|}
\hline
Параметр & Описание & Значение\\ \hline
\multicolumn{3}{|c|}{Q-learning \cite{sutton}}\\ \hline
$\alpha$ & скорость обучения & 0.6\\
$\gamma$ & дисконтный фактор & 0.1\\
$\varepsilon$ & вероятность исследования & 0.01\\ \hline
\multicolumn{3}{|c|}{Delayed Q-learning \cite{delayed}} \\ \hline
$m$ & период обновления & 5 \\
$\gamma$ & дисконтный фактор & 0.1\\
$\epsilon$ & бонусное вознаграждение & 0.2\\ \hline
\multicolumn{3}{|c|}{R-learning \cite{r-learning}}\\ \hline
$\alpha$ & скорость обновления вознаграждения $\rho$ & 0.5\\
$\beta$ & скорость обучения функции R & 0.35 \\
$\varepsilon$ & вероятность исследования & 0.25 \\ \hline
\multicolumn{3}{|c|}{Dyna \cite{sutton}}\\ \hline
$k$ & число обновлений & 20 \\
$\gamma$ & дисконтный фактор & 0.1 \\
$\varepsilon$ & вероятность исследования & 0.01 \\ \hline
\end{tabular}
\end{center}
\end{table}

\section{Результаты эксперимента}
\label{h-iff-results}

В табл. \ref{ga-rl} представлены результаты оптимизации (M)H-IFF с помощью различных алгоритмов. Результаты отсортированы по среднему значению целевой ФП лучших особей, полученных в результате 30 запусков соответствующих алгоритмов. Выделенные подсветкой алгоритмы реализованы с помощью предлагаемого метода c использованием различных алгоритмов обучения. Остальные результаты получены авторами статьи, причем алгоритмы PESA \cite{pesa} и PAES \cite{paes} являются алгоритмами многокритериальной оптимизации. Можно видеть, что предлагаемый метод в случае использования алгоритма обучения R-learning [8] позволяет преодолеть проблему остановки в локальном оптимуме столь же эффективно, как и метод PESA и более эффективно, чем метод PAES.

\begin{table}[ht]
\begin{center}
\caption{Результаты решения (M)H-IFF(A), отсортированные по убыванию среднего значения приспособленности особей} \label{ga-rl}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Алгоритм & Лучшее & Среднее & $\sigma$ & $\%$ одного оптимума & $\%$ двух оптимумов\\
\hline
\rowcolor{light-gray}
(1+10) ЭС + R-learning & 448 & 448.00 & 0.00 & 100 & 40\\
\rowcolor{light-gray}
ГА + R-learning & 448 & 448.00 & 0.00  & 100 & 10 \\
PESA \cite{pesa}   & 448 & 448.00 & 0.00  & 100 & 100 \\
\rowcolor{light-gray}
ГА + Q-learning & 448 & 435.61 & 32.94 & 87 & 3 \\
\rowcolor{light-gray}
ГА + Dyna & 448 & 433.07 & 38.07 & 80 & 0 \\
PAES \cite{paes}    & 448 & 418.13 & 50.68 & 74  & 43 \\
\rowcolor{light-gray}
ГА + Delayed Q-learning & 448 & 397.18 & 49.16 & 53 & 0\\
ГА + Random-FF-chooser & 384 & 354.67 & 29.24 & 0 & 0 \\
DCGA \cite{dcga}    & 448 & 323.93 & 26.54 & 3   & 0 \\
ГА & 384 & 304.53 & 27.55 & 0 & 0\\
SHC \cite{mh-iff} & 336 & 267.47 & 29.46 & 0   & 0 \\
(1+10) ЭС & 228 & 189.87 & 17.21 & 0 & 0\\\hline
\end{tabular}
\end{center}
\end{table}

В табл. \ref{es-r} отдельно рассмотрена оптимизация H-IFF с применением ЭС. Применяемая ЭС устроена таким образом, что решает задачу весьма неэффективно. Ни в одном из запусков ни одной из рассмотренных вариаций ЭС не удалось вырастить особь с максимальной приспособленностью. Однако применение предлагаемого метода позволило вырастить идеальные особи в 73\% запусков при настройке 1~+~1 ЭС и в 100\% запусков в других рассмотренных случаях.

\begin{table}[ht]
\begin{center}
\caption{Результаты решения H-IFF(A) с использованием ЭС и R-learning} \label{es-r}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Алгоритм & Лучшее & Среднее & $\sigma$ & $\%$ одного оптимума & $\%$ двух оптимумов\\
\hline
\rowcolor{light-gray}
(1+1) ЭС + R-learning & 448 & 403.49 & 59.48  & 73 & 10\\
(1+1) ЭС & 188 & 167.07 & 11.98 & 0 & 0\\ \hline
\rowcolor{light-gray}
(1+5) ЭС + R-learning & 448 & 448.00 & 0.00 & 100 & 37\\
(1+5) ЭС & 216 & 179.07 & 16.99 & 0 & 0 \\ \hline
\rowcolor{light-gray}
(1+10) ЭС + R-learning & 448 & 448.00 & 0.00 & 100 & 40\\
(1+10) ЭС & 228 & 189.87 & 17.21 & 0 & 0\\ \hline
\end{tabular}
\end{center}
\end{table}

Заметим, что целью предлагаемого метода является ускорение получения особей с высокими значениями целевой ФП. Приведенные результаты свидетельствуют о том, что метод успешно справляется с этой задачей. Невысокий процент нахождения обоих возможных оптимумов объясняется тем, что предлагаемый метод не проводит многокритериальную оптимизацию. В случае присутствия мешающих ФП подобное поведение является преимуществом разработанного метода, так как позволяет не учитывать такие ФП.

Рассмотрим последнюю часть проведенного эксперимента. В ней показано, что когда среди вспомогательных ФП есть не коррелирующие с целевой, предлагаемый метод будет эффективнее методов многокритериальной оптимизации. В табл. \ref{pesa-ii} представлены результаты решения MH-IFF и MH-IFF с мешающей ФП с помощью алгоритма PESA-II \cite{pesa-ii} и разработанного метода $(1 + 5)$-ЭС~+~R-learning. Можно видеть, что исходную задачу MH-IFF оба алгоритма решают идеально. Однако при наличии мешающей ФП с помощью алгоритма PESA-II не удается найти идеальное решение, в то время как предлагаемый метод позволяет выращивать лучшие возможные особи в 92\% запусков.

\begin{table}[ht]
\begin{center}
\caption{Сравнение разработанного метода с PESA-II} \label{pesa-ii}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Алгоритм & Лучшее & Среднее & $\sigma$ & $\%$ одного оптимума & $\%$ двух оптимумов\\
\hline
\multicolumn{6}{|c|}{Задача MH-IFF}\\ \hline
\rowcolor{light-gray}
(1+5) ЭС + R-learning & 448 & 448.00 & 0.00 & 100 & 37\\
PESA-II & 448 & 448.00 & 0.00 & 100 & 100\\ \hline
\multicolumn{6}{|c|}{Задача MH-IFF с мешающей ФП}\\ \hline
\rowcolor{light-gray}
(1+5) ЭС + R-learning & 448 & 439.45 & 36.32 & 92 & 45\\
PESA-II & 312 & 277.83 & 20.07 & 0 & 0 \\ \hline
\end{tabular}
\end{center}
\end{table}

\section{Выводы по главе \protect\ref{chapter3}}
Проведено сравнение предлагаемого метода с методами, основанными на сведении задач однокритериальной оптимизации к задачам многокритериальной оптимизации. Отмечено, что предлагаемый метод больше подходит для решения поставленной задачи скалярной оптимизации со вспомогательными критериями, свойства которых заранее не известны (\ref{task}), чем многокритериальная оптимизация. Это объясняется тем, что предлагаемый метод не ставит своей целью максимизацию всех вспомогательных критериев, что позволяет использовать его в случае, когда среди них есть критерии, отрицательно коррелирующие с целевой функцией.

Описана постановка модельной задачи H-IFF. Приведены результаты экспериментов, в ходе которых эта задача решается с использованием методов одно- и многокритериальной оптимизации, а также с использованием предлагаемого метода, основанного на различных алгоритмах обучения с подкреплением. Показано, что результаты работы предлагаемого метода совпадают с результатами работы наиболее эффективного метода многокритериальной оптимизации. Также на практике подтверждено, что предлагаемый метод не максимизирует все критерии, что не препятствует ускоренному получению оптимальных значений целевой функции. Показано, что в случае наличия мешающих критериев эффективность разработанного метода выше, чем эффективность метода многокритериальной оптимизации PESA-II.

Эксперимент показал устойчивость метода по отношению к характеру эволюционного алгоритма, к оптимизации которого он применяется. Эволюционные стратегии, показавшие худшие результаты по решению модельной задачи, под управлением обучения позволили стабильно выращивать лучшие возможные особи.
